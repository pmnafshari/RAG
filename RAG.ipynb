{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08300e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hitson/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.ctransformers import CTransformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1ec575",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import `ctransformers` package. Please install it with `pip install ctransformers`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/langchain_community/llms/ctransformers.py:64\u001b[39m, in \u001b[36mCTransformers.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ctransformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43mCTransformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTheBloke/Llama-2-7b-GGML\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/langchain_core/load/serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[39m, in \u001b[36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[39m\u001b[34m(values, _)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/langchain_core/utils/pydantic.py:184\u001b[39m, in \u001b[36mpre_init.<locals>.wrapper\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    181\u001b[39m             values[name] = field_info.default\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Codes/RAG/.venv/lib/python3.12/site-packages/langchain_community/llms/ctransformers.py:66\u001b[39m, in \u001b[36mCTransformers.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import `ctransformers` package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install ctransformers`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     71\u001b[39m config = values[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     72\u001b[39m values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     73\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     74\u001b[39m     model_type=values[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     **config,\n\u001b[32m     78\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: Could not import `ctransformers` package. Please install it with `pip install ctransformers`"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "llm = CTransformers(\n",
    "    model=\"TheBloke/Llama-2-7b-GGML\",\n",
    "    model_type=\"llama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b974770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Embeddings Dimentions\n",
    "embeddings_exmple = embed_texts(\"Hello, how are you?\")\n",
    "embeding_dim = embeddings_exmple.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37db4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize FAISS index \n",
    "index = faiss.IndexFlatL2(embeding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfccecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize InMemoryDocstore\n",
    "docstore = InMemoryDocstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an index-to-document mapping\n",
    "index_to_docstore_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47884314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the fiass vector store\n",
    "vector_store = FAISS(embedding_function=embed_texts, index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c029fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare documents\n",
    "documents = [\n",
    "    Document(page_content=\"RAG (Retrieval-Augmented Generation) is a method that combines a language model with an external database or documents, so the model can fetch relevant information before generating an answer.\"),\n",
    "    Document(page_content=\"RAG is commonly used in chatbots, question-answering systems, and search-based AI apps, because it reduces hallucination and improves reliability.\"),\n",
    "    Document(page_content=\"This approach helps the model produce more accurate, updated, and factual responses, especially when the needed information is not inside the model itself.\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13583558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed documents and add to the vector store\n",
    "texts = [doc.page_content for doc in documents]\n",
    "embeddings = get_embeddings(texts)\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    index.add(np.array([embedding], dtype=np.float32))\n",
    "    index_to_docstore_id[i] = documents[i].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faecd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a simple retriever\n",
    "def simple_retriever(query):\n",
    "  query_embedding = embed_texts([query])\n",
    "   D, I = index.search(query_embedding, k=1)\n",
    "    return index_to_docstore_id[I[0][0]] if len(I) > 0 and I[0][0] in index_to_docstore_id else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db501257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the RAG Chain\n",
    "class SimpleRetrieverlQA:\n",
    "    def __init__(self, retriever):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def run(self, query):\n",
    "        return self.retriever(query)\n",
    "        response = self.llm(f\"Context: {context}\\nQuestion: {query}\")\n",
    "  return response\n",
    "\n",
    "qa_chain = SimpleRetrieverlQA(llm = llm, retriever = simple_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ba2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Questions\n",
    "questions = [\"What is RAG?\", \"Why is RAG used?\", \"How does RAG work?\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Answers\n",
    "answers = qa_chain.run(questions)\n",
    "print(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
